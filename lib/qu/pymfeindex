#!/usr/bin/env python
from __future__ import division

import os
import sys
import datetime
from time import time
from optparse import OptionParser
import sqlite3

import platform
import subprocess
import re
from struct import *
from heapq import *
from zlib import *
from tempfile import TemporaryFile

D2n_dic = dict(A=0, T=3, C=2, G=1, a=0, t=3, c=2, g=1)
n2D_dic = {0:'A', 3:'T', 2:'C', 1:'G', 0:'a', 3:'t', 2:'c', 1:'g'}

class Chunk:
    '''Class to store information about a sorted segment of the data in
    a temporary file.  It records which file the data is stored in, and
    there the segment starts in that file.  It also provides an iterator
    allowing the stored data to be read back again.'''
    def __init__(self, item_len, f, pos):
        self.file = f
        self.file_pos = pos
        self.item_len = item_len
        self.count = 0
        self.compressed_len = 0

    def __iter__(self):
        '''generator function to iterate through the stored data'''
        if self.count == 0:
            return

        item_len = self.item_len
        max_seg_len = 8192
        decomp = decompressobj()
        carry_over = ""
        for seg_start in xrange(0, self.compressed_len, max_seg_len):
            self.file.seek(self.file_pos + seg_start, 0)
            seg_len = max_seg_len \
                      if seg_start + max_seg_len < self.compressed_len \
                      else self.compressed_len - seg_start
            seg = self.file.read(seg_len)
            data = carry_over + decomp.decompress(decomp.unconsumed_tail + seg)
            i = 0
            while i + item_len <= len(data):
                yield data[i:i + item_len]
                i += item_len
            carry_over = data[i:]

        data = carry_over + decomp.flush()
        for i in xrange(0, len(data), item_len):
            yield data[i:i + item_len]
                
class StrIter:
    '''Class to iterate through items packed into a bytearray, or a
    list of bytearrays.  Used to iterate through the last segment of the
    data without having to write it to a temporary file.'''
    def __init__(self, item_len, s):
        if isinstance(s, list):
            self.strs = s
        else:
            self.strs = [s]
        self.item_len = item_len

    def __iter__(self):
        for s in self.strs:
            for pos in xrange(0, len(s), self.item_len):
                yield str(s[pos:pos + self.item_len])

class MergeSorter:
    '''class to handle spilling excess data to temporary files and
    merging it when reading it back'''

    # limit individual file size to 2G
    max_file_size = 2000000000
    tmp_files = []
    chunks = []

    def __init__(self, l, k):
        self.len = l
        # Work out which bytes to radix sort on.  As the data gets stored
        # in position order, we only actually need to sort on mer_id
        # and direction (in the top bit of the record_id) as long
        # as the sort is stable.  We also don't need to bother with
        # the top 8 bits of mer_id as we bin on those bits already
        # so they'll be identical for everything in the same bin.
        self.sort_start_byte = l - calcsize('I') * 2 - int((k + 3 - 4) / 4)
        self.sort_end_byte = l - calcsize('I') * 2 + 1
        self.max_items = int(self.max_file_size / l)

    def new_tmp_file(self):
        f = TemporaryFile()
        self.tmp_files.append(f)
        return f

    def get_curr_file_num(self):
        if len(self.tmp_files) > 0:
            return self.tmp_files[-1]
        else:
            return self.new_tmp_file()

    def radix_sort_byte(self, item, byte):
        '''Stable radix sort on byte "byte" of entries in the bytearray "item"'''
        # Count number of items in each bin
        counts = [0 for x in xrange(256)]
        for p in xrange(byte, len(item), self.len):
            counts[item[p]] += 1

        # Work out where the items for each bin will go in the output array
        pos = [0]
        used_bins = 0
        for n, c in enumerate(counts):
            pos.append(pos[n] + c * self.len)
            used_bins += 1 if c > 0 else 0

        # Short-cut if there is only 1 bin as the input is already sorted
        if used_bins < 2:
            return item

        # Re-order into the new bytearray and return it.
        out = bytearray(item)
        for p in xrange(0, len(item), self.len):
            b = item[p + byte]
            out[pos[b]:pos[b] + self.len] = item[p:p + self.len]
            pos[b] += self.len

        return out

    def store(self, items):
        '''Sort and write items to a temporary file, and add to the list of
        segments to merge.'''
        f = self.get_curr_file_num()
        pos = f.tell()

        chunk = Chunk(self.len, f, pos)
        comp = compressobj(1)

        for b in xrange(256):
            # LSD radix sort items[b]
            for byte in reversed(xrange(self.sort_start_byte, self.sort_end_byte)):
                items[b] = self.radix_sort_byte(items[b], byte)

            # Write sorted data to the spill file
            sz = 8192
            num = int(len(items[b]) / self.len)
            i = 0
            while i < num:
                end = i + sz if i + sz < num else num

                shrunk = comp.compress(str(items[b][i * self.len:end * self.len]))
                f.write(shrunk)
                chunk.count += end - i
                pos += len(shrunk)
                chunk.compressed_len += len(shrunk)
                i = end
                if pos >= self.max_file_size:
                    # Finish compressed stream
                    remain = comp.flush()
                    f.write(remain)
                    chunk.compressed_len += len(remain)
                    # print "Wrote tmp file count = %d compressed size = %d" % (chunk.count, chunk.compressed_len)
                    # Start a new file
                    self.chunks.append(chunk)
                    f = self.new_tmp_file()
                    comp = compressobj(1)
                    pos = 0
                    chunk = Chunk(self.len, f, pos)
                
        if chunk.count > 0:
            # Finish compressed stream
            remain = comp.flush()
            f.write(remain)
            chunk.compressed_len += len(remain)
            self.chunks.append(chunk)
            # print "Wrote tmp file count = %d compressed size = %d" % (chunk.count, chunk.compressed_len)

    def store_last(self, items):
        '''Sort last items and add them to the list of segments to merge.
        In this case there is no need to save them.'''
        for b in xrange(256):
            # LSD radix sort items[b]
            for byte in reversed(xrange(self.sort_start_byte, self.sort_end_byte)):
                items[b] = self.radix_sort_byte(items[b], byte)
        self.chunks.append(StrIter(self.len, items))

    def __iter__(self):
        '''Use the heapq merge function to get an iterator that merges
        all the chunks together'''
        return merge(*self.chunks)


def print_usage():
    print '''
%s: Index DB for MFEprimer-2.0

Usage:

    %s -f human.genomic -k 9 -o index_db_name

Author: Wubin Qu <quwubin@gmail.com>
Last updated: 2012-9-28
    ''' % (os.path.basename(sys.argv[0]), os.path.basename(sys.argv[0]))

def optget():
    '''parse options'''
    parser = OptionParser()
    parser.add_option("-f", "--file", dest = "filename", help = "DNA file in fasta to be indexed")
    parser.add_option("-k", "--k", dest = "k", type='int', help = "K mer , default is 9", default = 9)
    parser.add_option("-o", "--out", dest = "out", help = "Index db file name")
    parser.add_option("-m", "--memlimit", dest = "memlimit", help = "Memory limit for sorting (megabytes)", default=200)

    (options, args) = parser.parse_args()

    if not options.filename:
        print_usage()
        exit()

    if not options.out:
        options.out = options.filename + '.sqlite3.db'

    if options.memlimit < 100:
        options.memlimit = 100

    return options

def parse_fasta_format(fh):
    '''
    A Fasta-format Parser return Iterator
    '''
    # Remove the comment and blank lines before the first record
    while True:
        line = fh.readline()
        if not line: return # Blank line

        line = line.strip()

        if line.startswith('>'):
            break

    while True:
        if not line.startswith('>'):
            raise ValueError("Records in Fasta files should start with '>' character")

        id, sep, desc = line[1:].partition(' ')

        seq_lines = []
        line = fh.readline()
        while True:
            if not line: break

            line = line.strip()

            if line.startswith('>'):
                break

            if not line:
                line = fh.readline()
                continue

            seq_lines.append(line.replace(' ', '').replace("\r", ''))
            line = fh.readline()

        yield (id, desc, ''.join(seq_lines))

        if not line: return

    assert False, 'Should not reach this line'

def insert_db(conn, mer_count, plus, minus):
    for mer_id in xrange(mer_count):
        conn.execute("insert into pos (mer_id, plus, minus) values (?, ?, ?)", \
                [mer_id, plus[mer_id], minus[mer_id]])

    conn.commit()

def update_db(conn, mer_count, plus, minus):
    for mer_id in xrange(mer_count):
        (plus_data, minus_data) = conn.execute("select plus, minus from pos where mer_id=?", [mer_id]).fetchone()
        if plus_data:
            if plus[mer_id]:
                plus_data += ';%s' % plus[mer_id]
            else:
                pass
        else:
            plus_data = plus[mer_id]

        if minus_data:
            if minus[mer_id]:
                minus_data += ';%s' % minus[mer_id]
            else:
                pass
        else:
            minus_data = minus[mer_id]

        conn.execute("update pos set plus=?, minus=? where mer_id=?", \
                [plus_data, minus_data, mer_id])

    conn.commit()

def baseN(num, b):
    '''convert non-negative decimal integer n to
    equivalent in another base b (2-36)'''
    return ((num == 0) and  '0' ) or ( baseN(num // b, b).lstrip('0') + "0123456789abcdefghijklmnopqrstuvwxyz"[num % b])

def int2DNA(num, k):
    seq = baseN(num, 4)
    return 'A' * (k-len(seq)) + (''.join([n2D_dic[int(base)] for base in seq]))

def DNA2int_2(seq):
    '''convert a sub-sequence/seq to a non-negative integer'''
    plus_mer = 0
    minus_mer = 0
    length = len(seq) - 1
    for i, letter in enumerate(seq):
        plus_mer += D2n_dic[letter] * 4 ** (length - i)
        minus_mer += (3 - D2n_dic[letter]) * 4 ** i

    return plus_mer, minus_mer

def seq_iter(seq, k):
    '''Iterate through the sequence returning (pos, plus_mer, minus_mer)
    tuples.'''
    mask = (4 ** k) - 1
    plus_mer  = -1
    minus_mer = -1
    pos = 0
    while pos < len(seq) - k + 1:
        if plus_mer < 0:
            # Calculate over full kmer
            try:
                plus_mer, minus_mer = DNA2int_2(seq[pos:pos + k])
                yield pos, plus_mer, minus_mer
                pos += 1
            except:
                # Unrecognised base.  Find the offending character and
                # skip past it
                i = pos + k - 1
                while i > pos and D2n_dic.has_key(seq[i]):
                    i -= 1
                pos = i + 1
                plus_mer = -1
        else:
            # Update
            c = seq[pos + k - 1]
            if D2n_dic.has_key(c):
                v = D2n_dic[c]
                plus_mer = (plus_mer << 2) & mask | v
                minus_mer = (minus_mer >> 2) | ((3 - v) << 2 * (k - 1))
                yield pos, plus_mer, minus_mer
                pos += 1
            else:
                # Last base unrecognised.  Skip entire kmer and restart
                pos += k
                plus_mer = -1
                

def DNA2int(seq):
    '''convert a sub-sequence/seq to a non-negative integer'''
    plus_mer = 0
    length = len(seq) - 1
    for i, letter in enumerate(seq):
        plus_mer += D2n_dic[letter] * 4 ** (length - i)

    return plus_mer

def store_to_db(conn, mer_id, next_mer_id, collation, record_names):
    if mer_id >= 0:
        plus = ";".join('%s:%s' % (record_names[x[0]], ",".join(str(y) for y in x[1])) for x in collation['+'])
        minus = ";".join('%s:%s' % (record_names[x[0]], ",".join(str(y) for y in x[1])) for x in collation['-'])
        conn.execute("insert into pos (mer_id, plus, minus)"
                     "values (?, ?, ?)", (mer_id, plus, minus))

    conn.executemany("insert into pos (mer_id, plus, minus)"
                     "values (?, ?, ?)",
                     ((x, '', '') for x in range(mer_id + 1, next_mer_id)))

def index(filename, k, dbname, memlimit):
    ''''''
    start = time()

    mer_count = 4**k

    conn = sqlite3.connect(dbname)
    cur = conn.cursor()
    cur.executescript('''
    drop table if exists pos;
    create table pos(
    mer_id integer primary key,
    plus text,
    minus text
    );''')


    count = 0
    record_num = 0
    record_names = []

    # Format for packing data.  By using big-endian byte ordering,
    # we get the result that a simple sort will put everything in the order
    # we want.
    if k > 16:
        fmt = '>QII'
    else:
        fmt = '>III'

    fmt_len = calcsize(fmt)
    max_count = memlimit * 2**20 // fmt_len
    sorter = MergeSorter(fmt_len, k)

    kshift = 2 * k - 8 if k > 4 else 0

    # Accumulator for packed data items
    items = [bytearray("") for x in xrange(256)]

    # Get data and sort
    for record_id, record_desc, fasta_seq in parse_fasta_format(open(filename)):
        is_empty = False
        print record_id
        record_names.append(record_id)
        record_num = len(record_names) - 1

        for i, plus_mer_id, minus_mer_id in seq_iter(fasta_seq, k):

            plus_bin = plus_mer_id >> kshift & 255
            minus_bin = minus_mer_id >> kshift & 255
            items[plus_bin].extend(pack(fmt, plus_mer_id, record_num, i + k - 1))
            items[minus_bin].extend(pack(fmt, minus_mer_id, record_num | 0x80000000, i))
            count += 2

            if count >= max_count:
                # print "@ %s : flushing out data" % str(datetime.timedelta(seconds=(time() - start)))
                sorter.store(items)
                # print "@ %s : flushed out data" % str(datetime.timedelta(seconds=(time() - start)))
                # items = []
                items = [bytearray("") for x in xrange(256)]
                count = 0
                
    # Don't forget the last bit
    # print "@ %s : flushing out last data" % str(datetime.timedelta(seconds=(time() - start)))
    sorter.store_last(items)
    # print "@ %s : flushed out last data" % str(datetime.timedelta(seconds=(time() - start)))

    # Merge it all together
    last_kmer = -1
    last_collation = { '+' : [], '-' : [] }

    # print "@ %s : starting merge phase" % str(datetime.timedelta(seconds=(time() - start)))
    for item in sorter:
        mer_id, record_num, pos = unpack(fmt, item)
        if record_num < 0x80000000:
            dirn = '+'
        else:
            record_num &= 0x7fffffff
            dirn = '-'
        
        if mer_id != last_kmer:
            store_to_db(conn, last_kmer, mer_id, last_collation, record_names)
            last_kmer = mer_id
            last_collation = { '+' : [], '-' : [] }

        if (len(last_collation[dirn]) == 0
            or last_collation[dirn][-1][0] != record_num):
            last_collation[dirn].append((record_num, []))
        last_collation[dirn][-1][1].append(pos)

    store_to_db(conn, last_kmer, mer_count, last_collation, record_names)
    conn.commit()

    print "Time used: %s" % str(datetime.timedelta(seconds=(time() - start)))
    print 'Done.'

def main():
    '''main'''
    options = optget()
    index(options.filename, options.k, options.out, options.memlimit)

if __name__ == "__main__":
    main()

